# Multimodal LLM Interpretability

Этот репозиторий является решением тестового задания зимы 2025 в TLab.

## Оглавление
- [Установка и настройка](#установка-и-настройка)
- [Использование](#)
  - [Logit Lens](#1-logit-lens)
- [Контакты](#контакты)

## Установка и настройка

### Prerequisites
- Убедитесь, что у вас установлен Python 3.8+ и `pip`.

### Шаги
1. **Клонируйте репозиторий:**
   ```bash
   git clone https://github.com/wolkendolf/tlab_MultimodalLLM
   cd tlab_MultimodalLLM
   ```

2. **Установите необходимые пакеты Python**:
    ```bash
    pip install -r requirements.txt
    ```
## Использование

### 1. Logit Lens
За основу была взята статья [Towards Interpreting Visual Information Processing in Vision-Language Models](https://arxiv.org/abs/2410.07149).

* `scripts/logit_lens/create_logit_lens.py` код запускает модель и создает интерактивный HTML файл для иллюстрации работы logit lens.

### 2.

## Контакты
Tg: [@Dan_i_il](@Dan_i_il)
