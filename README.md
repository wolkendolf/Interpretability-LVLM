# Multimodal LLM Interpretability

Этот репозиторий является решением тестового задания зимы 2025 в TLab.

## Оглавление
- [Multimodal LLM Interpretability](#multimodal-llm-interpretability)
  - [Оглавление](#оглавление)
  - [Установка и настройка](#установка-и-настройка)
    - [Prerequisites](#prerequisites)
    - [Шаги](#шаги)
  - [Использование](#использование)
    - [1. Logit Lens](#1-logit-lens)
    - [2.](#2)
  - [Контакты](#контакты)

## Установка и настройка

### Prerequisites
- Убедитесь, что у вас установлен Python 3.8+ и `pip`.

### Шаги
1. **Клонируйте репозиторий:**
   ```bash
   git clone https://github.com/wolkendolf/tlab_MultimodalLLM
   cd tlab_MultimodalLLM
   ```

2. **Установите необходимые пакеты Python**:
    ```bash
    pip install -r requirements.txt
    ```

## Использование
В быстро развивающемся ландшафте искусственного интеллекта многомодальные большие языковые модели становятся важной областью интереса. Эти модели, позволяющие объединить различные виды входных данных, становятся всё более популярными. Однако понимание их внутренних механизмов остается сложной задачей. В данной работе я исследую один из подходов анализа их внутренней работы - `Logit Lens`.

### 1. Logit Lens
LLM работает с двумя пространствами - исходным (vocab space) и латентным (embedding space), причем латентное пространство имеет меньшую размерность. Поскольку наша цель - лучше понимать процессы внутри нейронной сети, мы хотели бы уметь извлекать информацию из произвольного скрытого слоя. С этим нам поможет *Logit lens* - прием в исследовании интерпретируемости LLM, который фокусируется на том, во что «верит» LLM после каждого шага обработки, а не на том, как он обновляет это убеждение внутри шага.

В статье [Towards Interpreting Visual Information Processing in Vision-Language Models](https://arxiv.org/abs/2410.07149) есть хорошая иллюстрация этого метода (см. цифру 2 на картинке):
![logit-lens](./logit_lens_princ.jpg)
Суть заключается в том, что мы как бы через "микроскоп" ("lens") смотрим на то, как меняется ответ модели ("logit") слой за слоем.

В своей работе я провожу исследования с моделью `llava-1.5-7b`. 

* `python 3 scripts/create_logit_lens.py` код запускает модель и создает интерактивный HTML файл для иллюстрации работы logit lens.



### 2.

## Контакты
Tg: [@Dan_i_il](@Dan_i_il)
